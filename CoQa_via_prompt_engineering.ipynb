{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyWAnEj5xVK9Yn4i+p1Jvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_04/blob/main/CoQa_via_prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply zero-shot and few-shot learning with pretrained Language Models on the [Conversational Question Answering Challenge (CoQA) dataset](https://stanfordnlp.github.io/coqa/)"
      ],
      "metadata": {
        "id": "VY_j68uubrPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh_qmfD7E6rg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "import json\n",
        "\n",
        "import time\n",
        "\n",
        "import re\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_04\"\n",
        "COQA_DEV_SET=\"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "API_ACCESS=\"API_access_info.json\"\n",
        "\n",
        "COQA_EVALUATION_SCRIPT=\"https://nlp.stanford.edu/data/coqa/evaluate-v1.0.py\""
      ],
      "metadata": {
        "id": "nUXQ770XFvoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive, as usual"
      ],
      "metadata": {
        "id": "DRQXkWO4FQWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "hetH6XP2FT7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "RjIgP6VoFYI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the CoQa development set"
      ],
      "metadata": {
        "id": "AghkNU8uG4Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os.path.basename(COQA_DEV_SET)):\n",
        "    !wget {COQA_DEV_SET}\n",
        "else:\n",
        "    print(\"CoQa development dataset already downloaded...\")"
      ],
      "metadata": {
        "id": "FtrrjxPnFnMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read and explore the development set"
      ],
      "metadata": {
        "id": "7a-H6h3eHE__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.basename(COQA_DEV_SET), 'r') as inputFile:\n",
        "    dev_set = json.load(inputFile)"
      ],
      "metadata": {
        "id": "mMMCBPDmF_Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_set.keys()"
      ],
      "metadata": {
        "id": "q0VOs2XWGzhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dev_set['data'])"
      ],
      "metadata": {
        "id": "O542j_sUG1ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the evaluation script"
      ],
      "metadata": {
        "id": "QFPNa1cMPKKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os.path.basename(COQA_EVALUATION_SCRIPT)):\n",
        "    !wget {COQA_EVALUATION_SCRIPT}\n",
        "else:\n",
        "    print(\"Evaluation script already downloaded...\")"
      ],
      "metadata": {
        "id": "fxND0KaNPKKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create templates for zero-shot and few-shot learning"
      ],
      "metadata": {
        "id": "DtV3HgVPHYUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_PROMPT = \"Read the text, answer the questions and transcribe the text portion supporting your answer:\\n\\n\"\n",
        "TASK_PROMPT_NO_TRANSCRIPTION = \"Read the text and answer the questions:\\n\\n\"\n",
        "\n",
        "\n",
        "ZERO_SHOT_FIRST_QUESTION_TEMPLATE=\"Text: {}\\n\\nQuestion: {} Answer the question and transcribe the sentence where you found it.\"\n",
        "ZERO_SHOT_NEXT_QUESTIONS_TEMPLATE=\"\\nAnswer: {}\\nTranscription: {}\\n\\nQuestion: {}\"\n",
        "\n",
        "ZERO_SHOT_FIRST_QUESTION_TEMPLATE_NO_TRANSCRIPTION=\"Text: {}\\n\\nQuestion: {}\"\n",
        "ZERO_SHOT_NEXT_QUESTIONS_TEMPLATE_NO_TRANSCRIPTION=\"\\nAnswer: {}\\n\\nQuestion: {}\"\n",
        "\n",
        "\n",
        "FEW_SHOT_TEMPLATE=\"Example text: {}\\n\\nExample question: {}\\nExample answer: {}\\nExample transcription: {}\\n\\n\\n\\nText: {}\\n\\nQuestion: {}\"\n",
        "FEW_SHOT_SEQUENCE_TEMPLATE=\"Text: {}\\n\\nQuestion: {}\"\n",
        "FEW_SHOT_SEQUENCE_ADDITIONAL_QUESTION_TEMPLATE = \"\\nAnswer: {}\\nTranscription: {}\\n\\nQuestion: {}\"\n",
        "\n",
        "FEW_SHOT_TEMPLATE_NO_TRANSCRIPTION=\"Example text: {}\\n\\nExample question: {}\\nExample answer: {}\\n\\n\\n\\nText: {}\\n\\nQuestion: {}\"\n",
        "FEW_SHOT_SEQUENCE_ADDITIONAL_QUESTION_TEMPLATE_NO_TRANSCRIPTION = \"\\nAnswer: {}\\n\\nQuestion: {}\""
      ],
      "metadata": {
        "id": "c9h9-S1JSkqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEW_SHOT_QUERY_TYPE=\"few_shot\"\n",
        "ZERO_SHOT_QUERY_TYPE=\"zero_shot\""
      ],
      "metadata": {
        "id": "2mgBdoijkfUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results filename format:   \n",
        "\n",
        "```\n",
        "    test_<llama|text-davinci-003|code-davinci-002>_<few_shot|zero_shot>_<YYYYMMDD_HHMMSS>.json\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QI07qUhHdmNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_RESULTS_FILENAME_FORMAT=\"test_{}_{}_{}.json\""
      ],
      "metadata": {
        "id": "jxuGRXtpdgWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA_API_DATA_PACKAGE={\"prompt\": None,\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"top_p\": 1,\n",
        "                        \"max_length\": 100}\n",
        "\n",
        "OPENAI_API_QUERY_PARAMS={\"model\": \"code-davinci-002\",\n",
        "                         \"prompt\": None,\n",
        "                         \"temperature\": 0,\n",
        "                         \"max_tokens\": 100,\n",
        "                         \"top_p\": 1,\n",
        "                         \"frequency_penalty\": 0,\n",
        "                         \"presence_penalty\": 0}"
      ],
      "metadata": {
        "id": "Hx6BBq2hTn3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA_RESPONSE_REGEX=\".*[\\n\\r]*[a|A]nswer:(.+)[\\n\\r].*[t|T]ranscription[s]?:(.+)[\\n\\r]?\"\n",
        "LLAMA_RESPONSE_EMBEDDED_TRANSCRIPTION=\".*[\\n\\r]*[a|A]nswer:(.+)[\\.](.+)[\\n\\r]?\"\n",
        "LLAMA_RESPONSE_NO_TRANSCRIPTION_REGEX=\".*[\\n\\r]*[a|A]nswer:(.+)[\\n\\r]?\""
      ],
      "metadata": {
        "id": "uZontYbitEpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions to access the Language Models APIs"
      ],
      "metadata": {
        "id": "oXHKyImnOYS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_request_prompt(query_type, i, prompt_text, request_prompt, example_entry, test_entry, current_responses, ask_transcription):\n",
        "\n",
        "    if query_type == FEW_SHOT_QUERY_TYPE:\n",
        "        if i == 0:\n",
        "            #\n",
        "            # First time the prompt contains an example\n",
        "            #\n",
        "\n",
        "            if ask_transcription:\n",
        "                request_prompt = prompt_text + FEW_SHOT_TEMPLATE.format(example_entry['story'], \n",
        "                                                                        example_entry['questions'][0]['input_text'],\n",
        "                                                                        example_entry['answers'][0]['input_text'],\n",
        "                                                                        example_entry['answers'][0]['span_text'],\n",
        "                                                                        test_entry['story'],\n",
        "                                                                        test_entry['questions'][i]['input_text'])\n",
        "            else:\n",
        "                request_prompt = prompt_text + FEW_SHOT_TEMPLATE_NO_TRANSCRIPTION.format(example_entry['story'], \n",
        "                                                                                         example_entry['questions'][0]['input_text'],\n",
        "                                                                                         example_entry['answers'][0]['input_text'],\n",
        "                                                                                         test_entry['story'],\n",
        "                                                                                         test_entry['questions'][i]['input_text'])\n",
        "        else:\n",
        "            #\n",
        "            # For all the subsequent questions, the prompt will accumulate the answers, as the questions are\n",
        "            # conversational ― i.e. they build in one another.\n",
        "            #\n",
        "\n",
        "            if i == 1:\n",
        "                request_prompt = prompt_text + FEW_SHOT_SEQUENCE_TEMPLATE.format(test_entry['story'],\n",
        "                                                                                    test_entry['questions'][i - 1]['input_text'])\n",
        "\n",
        "            if ask_transcription:\n",
        "                request_prompt += FEW_SHOT_SEQUENCE_ADDITIONAL_QUESTION_TEMPLATE.format(current_responses[i - 1]['answer'],\n",
        "                                                                                        current_responses[i - 1]['transcription'],\n",
        "                                                                                        test_entry['questions'][i]['input_text'])\n",
        "            else:\n",
        "                request_prompt += FEW_SHOT_SEQUENCE_ADDITIONAL_QUESTION_TEMPLATE_NO_TRANSCRIPTION.format(current_responses[i - 1]['answer'],\n",
        "                                                                                                         test_entry['questions'][i]['input_text'])\n",
        "    elif query_type == ZERO_SHOT_QUERY_TYPE:\n",
        "        if i == 0:\n",
        "            #\n",
        "            # First time only contains the text and the question\n",
        "            #\n",
        "\n",
        "            if ask_transcription:\n",
        "                request_prompt = prompt_text + ZERO_SHOT_FIRST_QUESTION_TEMPLATE.format(test_entry['story'],\n",
        "                                                                                        test_entry['questions'][i]['input_text'])\n",
        "            else:\n",
        "                request_prompt = prompt_text + ZERO_SHOT_FIRST_QUESTION_TEMPLATE_NO_TRANSCRIPTION.format(test_entry['story'],\n",
        "                                                                                                         test_entry['questions'][i]['input_text'])\n",
        "        else:\n",
        "            #\n",
        "            # For all the subsequent questions, the prompt will accumulate the answers, as the questions are\n",
        "            # conversational ― i.e. they build in one another.\n",
        "            #\n",
        "\n",
        "            if ask_transcription:\n",
        "                request_prompt += ZERO_SHOT_NEXT_QUESTIONS_TEMPLATE.format(current_responses[i - 1]['answer'],\n",
        "                                                                           current_responses[i - 1]['transcription'],\n",
        "                                                                           test_entry['questions'][i]['input_text'])\n",
        "            else:\n",
        "                request_prompt += ZERO_SHOT_NEXT_QUESTIONS_TEMPLATE_NO_TRANSCRIPTION.format(current_responses[i - 1]['answer'],\n",
        "                                                                                            test_entry['questions'][i]['input_text'])\n",
        "\n",
        "\n",
        "    return request_prompt"
      ],
      "metadata": {
        "id": "Ff1jIZCAlgPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_llama(test_entry, add_prompt=True, query_type=FEW_SHOT_QUERY_TYPE, example_entry=None, ask_transcription=False):\n",
        "\n",
        "    test_entry_start_time = time.time()\n",
        "\n",
        "    llama_responses = []\n",
        "\n",
        "    if add_prompt:\n",
        "        if ask_transcription:\n",
        "            prompt_text = TASK_PROMPT\n",
        "        else:\n",
        "            prompt_text = TASK_PROMPT_NO_TRANSCRIPTION\n",
        "    else:\n",
        "        prompt_text = \"\"\n",
        "\n",
        "    request_prompt = \"\"\n",
        "\n",
        "    for i in range(len(test_entry['questions'])):\n",
        "\n",
        "        request_prompt = build_request_prompt(query_type, i, prompt_text, request_prompt, example_entry, test_entry, llama_responses, ask_transcription)\n",
        "\n",
        "            \n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"QUESTION #{}\".format(i))\n",
        "        print(\"--------------------------------------------\\n\")\n",
        "        print(request_prompt)\n",
        "\n",
        "        request_data = LLAMA_API_DATA_PACKAGE\n",
        "        request_data['prompt'] = request_prompt\n",
        "\n",
        "        request_start_time = time.time()\n",
        "\n",
        "        r = requests.post(f\"{access_info['LLAMA_API_ENDPOINT']}/complete\", json=request_data)\n",
        "\n",
        "        if r.ok:\n",
        "            response=r.json()\n",
        "\n",
        "            request_uuid=response[\"request_uuid\"]\n",
        "\n",
        "            ready = False\n",
        "            while not ready:\n",
        "                r = requests.get(f\"{access_info['LLAMA_API_ENDPOINT']}/get_result/{request_uuid}\")\n",
        "                response = r.json()\n",
        "                ready = response['ready']\n",
        "                if ready:\n",
        "                    print(response['generated_text'])\n",
        "\n",
        "                    elapsed_time = time.time() - request_start_time\n",
        "\n",
        "                    print(\"\\n>> Request elapsed time: {:.3f}\".format(elapsed_time))\n",
        "\n",
        "                    if ask_transcription:\n",
        "                        m = re.match(LLAMA_RESPONSE_REGEX, response['generated_text'])\n",
        "                    else:\n",
        "                        m = re.match(LLAMA_RESPONSE_NO_TRANSCRIPTION_REGEX, response['generated_text'])\n",
        "\n",
        "                    if m is None:\n",
        "                        print(\"Try another match...\")\n",
        "\n",
        "                        m = re.match(LLAMA_RESPONSE_EMBEDDED_TRANSCRIPTION, response['generated_text'])\n",
        "\n",
        "                    if m is not None:\n",
        "                        answer_text = m.group(1).strip()\n",
        "                        transcription_text = \"\"\n",
        "\n",
        "                        if ask_transcription and (len(m.groups()) > 1):\n",
        "                            transcription_text = m.group(2).strip()\n",
        "\n",
        "\n",
        "                        llama_responses.append({'id': test_entry['id'],\n",
        "                                                'turn_id': test_entry['questions'][i]['turn_id'],\n",
        "                                                'answer': answer_text, \n",
        "                                                'transcription': transcription_text})\n",
        "                    else:\n",
        "                        print(\"No match!!!\")\n",
        "\n",
        "                        for byte in bytes(response['generated_text'], 'utf-8'):\n",
        "                            print(byte, end=\" \")\n",
        "\n",
        "                        #\n",
        "                        # Add empty response to avoid breaking the treatment.\n",
        "                        #\n",
        "\n",
        "                        llama_responses.append({'id': test_entry['id'],\n",
        "                                                'turn_id': test_entry['questions'][i]['turn_id'],\n",
        "                                                'answer': \"\", \n",
        "                                                'transcription': \"\"})\n",
        "                        \n",
        "\n",
        "                    if elapsed_time < 20:\n",
        "                        print(\"Wait 10 seconds to avoid getting a 429 error...\")\n",
        "\n",
        "                        time.sleep(10)\n",
        "\n",
        "                    break\n",
        "\n",
        "                # Wait 10 seconds before checking again\n",
        "\n",
        "                time.sleep(10)\n",
        "\n",
        "            print(\"\\n\\n\")\n",
        "        else:\n",
        "            print(\"\\n\\nREQUEST FAILED!!!\\n\\n\")\n",
        "\n",
        "    print(\"Elapse total of {:.3f} s to execute all the {} queries\".format(time.time() - test_entry_start_time, len(test_entry['questions'])))\n",
        "\n",
        "    return llama_responses"
      ],
      "metadata": {
        "id": "pn9k5WpSOz7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_test(test_set_filename, test_set_data, selected_entries, llm=\"llama\", test_parameters={'example_entry': None,\n",
        "                                                                                                   'query_type': FEW_SHOT_QUERY_TYPE,\n",
        "                                                                                                   'add_prompt': True,\n",
        "                                                                                                   'ask_transcription': False}):\n",
        "    \n",
        "    test_start_time = time.time()\n",
        "\n",
        "    test_responses = []\n",
        "\n",
        "    test_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_filename = TEST_RESULTS_FILENAME_FORMAT.format(llm, test_parameters['query_type'], test_timestamp)\n",
        "\n",
        "    executed_test = {'timestamp': test_timestamp,\n",
        "                     'set': test_set_filename,\n",
        "                     'set_entries': [int(a) for a in selected_entries],\n",
        "                     'configuration': test_parameters,\n",
        "                     'answers': None}\n",
        "\n",
        "    if llm == \"llama\":\n",
        "        for test_entry in [test_set_data['data'][i] for i in selected_entries]:\n",
        "            test_responses += query_llama(test_entry, **test_parameters)\n",
        "\n",
        "            # Save the results so far just to make sure they are not lost...\n",
        "\n",
        "            executed_test['answers'] = test_responses\n",
        "\n",
        "            print(executed_test)\n",
        "\n",
        "            with open(results_filename, \"w\") as outputFile:\n",
        "                json.dump(executed_test, outputFile, indent=4)\n",
        "\n",
        "    print(\"Total elapsed time: {}\".format(time.time() - test_start_time))\n",
        "\n",
        "    return results_filename"
      ],
      "metadata": {
        "id": "gH4Qflzy7vfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define LLAMA test API endpoint"
      ],
      "metadata": {
        "id": "lZ-hTd8LU6Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(API_ACCESS) as inputFile:\n",
        "    access_info = json.load(inputFile)"
      ],
      "metadata": {
        "id": "9dcIt5BVU-W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select 5 entries to test\n",
        "\n",
        "Leave the first story as the few-shot example."
      ],
      "metadata": {
        "id": "aIE6yXDYVizO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entries_to_test = np.random.choice(list(range(1, len(dev_set['data']))), 5, replace=False)"
      ],
      "metadata": {
        "id": "dhhbrAFTgZdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create a reference dataset containing only the tested queries"
      ],
      "metadata": {
        "id": "3tj9drBT7XoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dataset_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "metadata": {
        "id": "8dk7lmEE7aCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dataset = {\"version\": 1.0,\n",
        "                     \"data\": dev_set['data'][entries_to_test]}"
      ],
      "metadata": {
        "id": "4Eimespj7XoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REFERENCE_DATASET=\"reference_dataset_{}.json\".format(reference_dataset_timestamp)"
      ],
      "metadata": {
        "id": "aWtE91op7XoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(REFERENCE_DATASET, \"w\") as outputFile:\n",
        "    json.dump(reference_dataset, outputFile, indent=4)"
      ],
      "metadata": {
        "id": "7ibIvnxc7XoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now execute the test sequence for LLaMA"
      ],
      "metadata": {
        "id": "zsBP_xus7yDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_files = []"
      ],
      "metadata": {
        "id": "0u8bOzrlxMUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the tests using few-shot setup with prompt"
      ],
      "metadata": {
        "id": "fHcdCLqFwWwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_parameters={'example_entry': dev_set['data'][0],\n",
        "                 'query_type': FEW_SHOT_QUERY_TYPE,\n",
        "                 'add_prompt': True,\n",
        "                 'ask_transcription': False}\n",
        "\n",
        "test_results_files.append(execute_test(os.path.basename(COQA_DEV_SET), dev_set, entries_to_test, test_parameters=test_parameters))"
      ],
      "metadata": {
        "id": "hcTZkDwIc_M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now execute using zero-shot setup, no prompt"
      ],
      "metadata": {
        "id": "_deMT_OswgLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_parameters={'example_entry': dev_set['data'][0],\n",
        "                 'query_type': ZERO_SHOT_QUERY_TYPE,\n",
        "                 'add_prompt': False,\n",
        "                 'ask_transcription': False}\n",
        "\n",
        "test_results_files.append(execute_test(os.path.basename(COQA_DEV_SET), dev_set, entries_to_test, test_parameters=test_parameters))"
      ],
      "metadata": {
        "id": "KwS907a7ofuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, execute the tests using few-shot setup without prompt"
      ],
      "metadata": {
        "id": "sBt2kQelw12i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_parameters={'example_entry': dev_set['data'][0],\n",
        "                 'query_type': FEW_SHOT_QUERY_TYPE,\n",
        "                 'add_prompt': False,\n",
        "                 'ask_transcription': False}\n",
        "\n",
        "test_results_files.append(execute_test(os.path.basename(COQA_DEV_SET), dev_set, entries_to_test, test_parameters=test_parameters))"
      ],
      "metadata": {
        "id": "wkfzFhJFw12i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### And finaly execute using zero-shot setup, with prompt"
      ],
      "metadata": {
        "id": "mqB04-gaw12j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_parameters={'example_entry': dev_set['data'][0],\n",
        "                 'query_type': ZERO_SHOT_QUERY_TYPE,\n",
        "                 'add_prompt': True,\n",
        "                 'ask_transcription': False}\n",
        "\n",
        "test_results_files.append(execute_test(os.path.basename(COQA_DEV_SET), dev_set, entries_to_test, test_parameters=test_parameters))"
      ],
      "metadata": {
        "id": "kVvC6ZkVw12j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now execute the evaluation script for the executed tests"
      ],
      "metadata": {
        "id": "hVldHdNGFxae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create a reference dataset containing only the tested queries"
      ],
      "metadata": {
        "id": "mlEdz98ZaZkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_dataset = {\"version\": 1.0,\n",
        "                     \"data\": dev_set['data'][entries_to_test]}"
      ],
      "metadata": {
        "id": "EgTLhDdHafus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REFERENCE_DATASET=\"reference_dataset.json\""
      ],
      "metadata": {
        "id": "-_cGB19ZalMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(REFERENCE_DATASET, \"w\") as outputFile:\n",
        "    json.dump(reference_dataset, outputFile, indent=4)"
      ],
      "metadata": {
        "id": "za1VKYUjafXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments on the evaluation script\n",
        "\n",
        "The evaluation script computes Exact Match and F1 between the predicted answer and the gold standard.\n",
        "\n",
        "One comment is that the model will be penalized if it produces verbose answers, even if it contains the correct answer."
      ],
      "metadata": {
        "id": "SsHaP40ecHxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for test_result in test_results_files:\n",
        "\n",
        "    print(\"\\n\\n\\n---------------------------------------------------\")\n",
        "    print(\"Evaluation results for {}...\".format(test_result))\n",
        "    print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "    !python evaluate-v1.0.py --data-file {REFERENCE_DATASET} --pred-file {test_result} --human"
      ],
      "metadata": {
        "id": "15z0fvKyYhTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_r3EgEuyEQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}